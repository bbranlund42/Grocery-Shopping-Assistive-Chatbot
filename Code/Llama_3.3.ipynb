{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"What are the common kinds of apples?\"\n",
    "\n",
    "context = \"\"\"You are a helpful assistant that helps shoppers \n",
    "in a grocery store with a variety of issues. Please refrain \n",
    "from discussing anything but grocery-store and product-related \n",
    "questions and topics.\"\"\"\n",
    "\n",
    "'''history = f\"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}<|eot_id|\n",
    "<|start_header_id|>assistant<|end_header_id|\n",
    "{response_text}<eot_id|\n",
    "\"\"\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the native inference API to send a text message to Meta Llama 3.\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-2\")\n",
    "\n",
    "# Set the model ID, e.g., Llama 3 70b Instruct.\n",
    "model_id = \"meta.llama3-3-70b-instruct-v1:0\"\n",
    "\n",
    "# Embed the prompt in Llama 3's instruction format.\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{context}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_gen_len\": 128,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a wide variety of apples here in our store. Some of the most common kinds of apples include:\n",
      "\n",
      "1. Red Delicious: Known for their bright red skin and sweet taste.\n",
      "2. Gala: Sweet and crunchy, with a hint of vanilla flavor.\n",
      "3. Fuji: Sweet and juicy, with a crisp texture.\n",
      "4. Granny Smith: Tart and green, great for baking and cooking.\n",
      "5. Golden Delicious: Sweet and mild, great for snacking and salads.\n",
      "\n",
      "We also have some other varieties like Honeycrisp, McIntosh, and Braeburn, which are popular among our customers. Would you like to try any\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Decode the response body.\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Extract and print the response text.\n",
    "response_text = model_response[\"generation\"]\n",
    "print(response_text)\n",
    "\n",
    "# snippet-end:[python.example_code.bedrock-runtime.InvokeModel_MetaLlama3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tiktoken\n",
    "\n",
    "#encoding = tiktoken.get(\"meta.llama3-3-70b-instruct-v1:0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
