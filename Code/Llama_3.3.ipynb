{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "#prompt = \"What are some common problems someone may run into when shopping at a grocery store?\"\n",
    "prompt = \"Is a Tomato a fuit or a vegetable?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# snippet-start:[python.example_code.bedrock-runtime.InvokeModel_MetaLlama3]\n",
    "# Use the native inference API to send a text message to Meta Llama 3.\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-2\")\n",
    "\n",
    "# Set the model ID, e.g., Llama 3 70b Instruct.\n",
    "model_id = \"meta.llama3-3-70b-instruct-v1:0\"\n",
    "\n",
    "# Embed the prompt in Llama 3's instruction format.\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_gen_len\": 1024,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that has sparked debate for centuries!\n",
      "\n",
      "From a botanical standpoint, a tomato is technically a fruit. Here's why:\n",
      "\n",
      "1. **Definition of a fruit**: A fruit is the mature ovary of a plant, which contains seeds, while a vegetable is any other edible part of a plant, such as the leaves (like lettuce), stems (like celery), roots (like carrots), or tubers (like potatoes).\n",
      "2. **Tomato's structure**: A tomato is the edible, ripened ovary of a flower, containing seeds, which makes it a type of fruit called a \"berry\" (yes, you read that right - a berry!).\n",
      "3. **Botanical classification**: Tomatoes belong to the nightshade family (Solanaceae), which includes other fruits like peppers, eggplants, and okra.\n",
      "\n",
      "However, in culinary and everyday contexts, tomatoes are often referred to as a vegetable because they are savory rather than sweet and are often used in dishes in a similar way to other vegetables.\n",
      "\n",
      "In 1893, the US Supreme Court even weighed in on the matter in the case of Nix v. Hedden, ruling that tomatoes are vegetables for the purposes of tariffs and trade. This decision was based on the common usage of the terms \"fruit\" and \"vegetable\" in cooking and commerce, rather than botanical definitions.\n",
      "\n",
      "So, to summarize: from a botanical perspective, a tomato is a fruit, but in culinary and everyday contexts, it's often referred to as a vegetable.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Decode the response body.\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Extract and print the response text.\n",
    "response_text = model_response[\"generation\"]\n",
    "print(response_text)\n",
    "\n",
    "# snippet-end:[python.example_code.bedrock-runtime.InvokeModel_MetaLlama3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
